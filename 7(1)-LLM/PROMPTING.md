LLM Prompting 기법 비교 보고서

1. 실험 결과 요약

1.1 정답률 비교표

Prompting 기법 | 0-shot | 3-shot | 5-shot
Direct Prompting | 0.80 (80%) | 0.76 (76%) | 0.80 (80%)
CoT Prompting | 0.70 (70%) | 0.72 (72%) | 0.78 (78%)
My Prompting | 0.74 (74%) | 0.80 (80%) | 0.84 (84%)


2. CoT Prompting이 Direct Prompting에 비해 좋을 수 있는 이유

일반적으로 Chain-of-Thought (CoT) Prompting이 Direct Prompting보다 성능이 우수할 수 있는 이유는 다음과 같습니다:

참고: 본 실험에서는 일부 케이스에서 Direct Prompting이 더 높은 성능을 보였지만, 이는 모델이나 데이터셋의 특성에 따라 달라질 수 있습니다. 일반적으로 CoT는 복잡한 추론이 필요한 문제에서 더 효과적입니다.

2.1 단계별 추론 과정의 명시화
- Direct Prompting: 모델이 최종 답만 생성하도록 요구하여, 복잡한 수학 문제에서 중간 계산 과정을 생략하거나 잘못된 추론을 할 수 있습니다.
- CoT Prompting: 각 단계의 추론 과정을 명시적으로 보여주므로, 모델이 문제를 더 체계적으로 분해하고 해결할 수 있습니다.

2.2 오류 검출 및 수정 가능성
- CoT 방식에서는 각 단계의 결과를 확인할 수 있어, 중간 단계에서 발생한 오류를 식별하고 수정할 기회가 있습니다.
- Direct 방식은 최종 답만 제공하므로, 오류의 원인을 파악하기 어렵습니다.

2.3 복잡한 문제 해결 능력 향상
- 다단계 계산이 필요한 문제에서, CoT는 각 단계를 순차적으로 처리하도록 유도합니다.
- 예를 들어, "A를 계산한 후 B를 계산하고, 그 결과로 C를 계산"하는 문제에서 CoT는 각 단계를 명확히 구분하여 처리합니다.

2.4 Few-shot Learning의 효과 극대화
- Few-shot 예시에서 추론 과정을 보여주면, 모델이 유사한 패턴을 학습하여 새로운 문제에도 적용할 수 있습니다.
- Direct 방식은 답만 보여주므로, 문제 해결 방법론을 학습하기 어렵습니다.


3. My Prompting이 CoT에 비해 더 좋을 수 있는 이유

본인이 구현한 프롬프트 기법(Enhanced Step-by-Step CoT)이 기본 CoT보다 성능이 우수한 이유는 다음과 같습니다:

3.1 명시적인 단계 구조화
- 기본 CoT: 추론 과정을 자연어로 기술하지만, 단계 구분이 모호할 수 있습니다.
- My Prompting: 각 단계를 "Step:" 접두사로 명시적으로 표시하여, 모델이 단계별로 더 체계적으로 추론하도록 유도합니다.

3.2 구조화된 해결 프로세스 제시
For each step:
1. Identify what information you need
2. Perform the calculation clearly
3. State the result
4. Use the result in the next step if needed
- 위와 같은 명확한 프로세스를 제시함으로써, 모델이 일관된 방식으로 문제를 해결하도록 합니다.
- 기본 CoT는 자유로운 형식이지만, My Prompting은 더 구조화된 접근을 강제합니다.

3.3 중간 계산 결과의 명확한 표시
- 각 단계마다 "Step:" 라벨을 붙여 중간 계산 결과를 더 명확하게 구분합니다.
- 이는 모델이 이전 단계의 결과를 다음 단계에 정확히 활용하도록 돕습니다.

3.4 최종 답의 명확한 표시
- "Final Answer:" 라벨을 사용하여 최종 답을 명확히 구분합니다.
- 이는 extract_final_answer 함수가 답을 더 정확하게 추출할 수 있도록 돕습니다.

3.5 Few-shot 예시의 일관성
- 모든 예시에서 동일한 구조(Step: ... Final Answer: ...)를 사용하여, 모델이 일관된 패턴을 학습하도록 합니다.


4. 실험 방법론

4.1 데이터셋
- GSM8K: 초등학교 수준의 수학 단어 문제 데이터셋
- 테스트 샘플 수: 50개 (각 프롬프트 기법별)

4.2 모델
- llama-3.1-8b-instant (Groq API 사용)
- Temperature: 0.3

4.3 평가 지표
- 정답률 (Accuracy): 예측 답과 정답의 차이가 1e-5 미만인 경우 정답으로 간주


5. 결론

본 실험을 통해 다음을 확인할 수 있었습니다:

1. Few-shot Learning의 효과: My Prompting의 경우 예시가 많을수록(0-shot: 74% → 3-shot: 80% → 5-shot: 84%) 성능이 향상되는 경향을 보입니다.

2. 구조화된 프롬프트의 중요성: My Prompting이 5-shot에서 84%로 가장 높은 성능을 달성했습니다. 이는 단계를 명시적으로 구조화하고 일관된 형식을 사용하는 것이 성능 향상에 도움이 됨을 보여줍니다.

3. 프롬프트 엔지니어링의 효과: 단순히 추론 과정을 보여주는 것(CoT)보다, 명확한 단계 구조와 일관된 형식을 제시하는 것(My Prompting)이 더 효과적일 수 있습니다.

